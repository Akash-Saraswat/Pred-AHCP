{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET DEVELOPMENT AND FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Akash\\Downloads\\run\\run_500\\P_N.csv\")\n",
    "\n",
    "font_size = 18\n",
    "\n",
    "positive = df[df['Label'] == 1]\n",
    "negative = df[df['Label'] == 0]\n",
    "\n",
    "#plotting the \"Sequence\" column array length for 0 and 1 labels in the df dataframe with 1200 pixels in width and 600 pixels in height\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist([positive['Sequence'].str.len(), negative['Sequence'].str.len()], bins=40, label=['Positive', 'Negative'])\n",
    "\n",
    "#setting the legend and font size of the plot\n",
    "plt.legend(fontsize=font_size)\n",
    "\n",
    "# setting the x and y axis labels\n",
    "plt.xlabel('Sequence Length', fontsize=font_size)\n",
    "plt.ylabel('Count', fontsize=font_size)\n",
    "\n",
    "#setting the font of the x and y tick labels\n",
    "plt.xticks(fontsize=font_size)\n",
    "plt.yticks(fontsize=font_size)\n",
    "\n",
    "figsize = (40, 30)\n",
    "\n",
    "\n",
    "# saving the plot in 1200 dpi\n",
    "plt.savefig(r\"C:\\Users\\Akash\\Downloads\\run\\run_500\\0_Sequence_Distribution.png\", dpi=1200, bbox_inches='tight')\n",
    "\n",
    "\n",
    "# Developing X and Y\n",
    "X = df.drop(['ID', 'Label', 'Sequence'], axis=1)\n",
    "Y = df[['Label']]\n",
    "Y = np.ravel(Y)\n",
    "\n",
    "cols_MI = X.columns\n",
    "\n",
    "\n",
    "#plotting X max values for each column\n",
    "X.max().plot(kind='bar')\n",
    "\n",
    "\n",
    "# isolating the X columns with maximum value greater than 1.0\n",
    "df1 = X.loc[:, (X.max() > 1.0)]\n",
    "\n",
    "#develop a for loop to divide each value in column by 100\n",
    "for i in df1.columns:\n",
    "    X[i] = df1[i].div(100)\n",
    "\n",
    "df1.describe()\n",
    "X.describe()\n",
    "\n",
    "#plotting X max values for each column\n",
    "X.max().plot(kind='bar')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import mutual_info_classif as MIC\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunction(X,Y, iteration):\n",
    "    random_numbers = random.sample(range(0, 1000000), 500)\n",
    "    mutual_info = []\n",
    "    for i in random_numbers:\n",
    "        mutual_info.append(mutual_info_classif(X,Y,random_state=i))\n",
    "    \n",
    "    #converting the list into a dataframe\n",
    "    df_list = pd.DataFrame(mutual_info)\n",
    "    #transposing the list to insert column names as index\n",
    "    df_list = df_list.T\n",
    "    df_list = df_list.set_index(cols_MI)\n",
    "    df_list = df_list.T\n",
    "\n",
    "    df_list_2 = df_list\n",
    "    df_list_2 = df_list_2.T\n",
    "\n",
    "    #Sorting by column \"mean\"\n",
    "    df_list_2['mean'] = df_list_2.mean(axis=1)\n",
    "\n",
    "    df_list_2 = df_list_2.sort_values(by=['mean'], ascending=False)\n",
    "\n",
    "    #Filtering MI values greater than 0.030\n",
    "    df_list_2 = df_list_2[df_list_2['mean'] > 0.03]\n",
    "    #print(df_list_2)\n",
    "\n",
    "    #Creating a subset of X\n",
    "    MI_score_filtered_X = X[df_list_2.index]\n",
    "\n",
    "    # creating new dataset for analysis\n",
    "    Dataset_etc = df[['ID', 'Label', 'Sequence']]\n",
    "    Dataset_MI_1 = pd.concat([Dataset_etc, MI_score_filtered_X], axis=1)\n",
    "    # create a folder named iteration\n",
    "\n",
    "    os.mkdir(r\"C:\\Users\\Akash\\Downloads\\run\\run_500\\run_\"+str(iteration))\n",
    "    # save the dataset in this newly created folder\n",
    "\n",
    "    Dataset_MI_1.to_csv(r\"C:\\Users\\Akash\\Downloads\\run\\run_500\\run_\"+str(iteration)+\"\\1_Dataset_MI_1.csv\",index=False)\n",
    "\n",
    "\n",
    "    mutual_info_features = []\n",
    "    index_list = ['Run' + str(i + 1) for i in range(df_list.shape[0])]\n",
    "    for i,row in df_list.iterrows():\n",
    "        row_data = row.to_dict()\n",
    "        row_data = dict(sorted(row_data.items(),key = lambda x:x[1],reverse=True))\n",
    "        temp_list = [i for i in row_data.keys()]\n",
    "        mutual_info_features.append(temp_list[:10])\n",
    "    \n",
    "\n",
    "\n",
    "    temp = {}\n",
    "    for index,row in zip(index_list,mutual_info_features):\n",
    "        temp[index] = row\n",
    "    df_mi = pd.DataFrame(temp)\n",
    "    df_mi = df_mi.T\n",
    "    df_mi\n",
    "    df_mi.to_csv(r\"C:\\Users\\Akash\\Downloads\\run\\run_500\\run_\"+str(iteration)+\"\\2_MI_Features.csv\",index=False)\n",
    "    \n",
    "\n",
    "    #Ananlysis: 2 Ranking\n",
    "    df_list_1 = df_list\n",
    "    df_list_1 = df_list_1.T\n",
    "    ranked_list = df_list_1.rank()\n",
    "\n",
    "    ranked_list['mean'] = ranked_list.mean(axis=1)\n",
    "\n",
    "    ranked_list = ranked_list.sort_values(by=['mean'], ascending=False)\n",
    "\n",
    "    dataset_VIF = pd.read_csv(r\"C:\\Users\\Akash\\Downloads\\run\\run_500\\run_\"+str(iteration) + \"\\2_Dataset_MI_1.csv\",index_col=0)\n",
    "\n",
    "    # Developing X and filling the empty values\n",
    "    X_VIF = dataset_VIF.drop(['ID', 'Label', 'Sequence'], axis=1)\n",
    "\n",
    "    #transposing dataset_VIF\n",
    "    X_VIF = X_VIF.T\n",
    "\n",
    "    # Sorting the index of X_VIF in ascending order\n",
    "    X_VIF = X_VIF.sort_index(axis=0, ascending=True)\n",
    "\n",
    "    X_VIF = X_VIF.T\n",
    "\n",
    "    def computeVIF(Xdata):\n",
    "        # VIF dataframe\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"feature\"] = Xdata.columns\n",
    "\n",
    "        #print(vif_data)\n",
    "\n",
    "        # calculating VIF for each feature\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(Xdata.values, i) for i in range(len(Xdata.columns))]\n",
    "        return vif_data\n",
    "    \n",
    "    #Computing VIF < 05\n",
    "    my_lists_1 = [list() for i in range(300)]\n",
    "\n",
    "    for i in range(len(my_lists_1)):\n",
    "        temp_1 = computeVIF(X_VIF)\n",
    "        temp_1 = temp_1.sort_values(by=[\"VIF\"],ascending=False).reset_index(drop=True)\n",
    "        if(temp_1['VIF'][0]>5):\n",
    "            temp_1 = temp_1.iloc[1:]\n",
    "            my_lists_1[i] = temp_1.loc[temp_1['VIF']<=5]['feature'].to_list()\n",
    "            X_VIF=X_VIF[temp_1['feature']]\n",
    "        else:\n",
    "            break\n",
    "    #X_VIF is getting updated by the features whose values are less than 10 because our compute vif\n",
    "    #function is returning only those features whose values are less than 10 for the VIF score\n",
    "\n",
    "    temp_1 = temp_1.sort_values(by=\"VIF\", ascending=False)\n",
    "    temp_1.head()\n",
    "    temp_1.to_csv(r\"C:\\Users\\Akash\\Downloads\\run\\run_500\\run_\"+str(iteration)+\"\\5_VIF_values.csv\",index=False)\n",
    "\n",
    "    Xdata_after_MI_1_VIF_iter = X[temp_1['feature']]\n",
    "    Xdata_after_MI_1_VIF_iter.shape\n",
    "\n",
    "    # creating new dataset for analysis \"dataset\" to make, \"Dataset\" for new dataset.\n",
    "    dataset_MI_1_VIF = pd.concat([Dataset_etc, Xdata_after_MI_1_VIF_iter], axis=1)\n",
    "    dataset_MI_1_VIF.to_csv(r\"C:\\Users\\Akash\\Downloads\\run\\run_500\\run_\"+ str(iteration) +\"\\7_dataset_MI_1_VIF.csv\",index=False)\n",
    "\n",
    "    Dataset_MI_1_VIF = pd.read_csv(r\"C:\\Users\\Akash\\Downloads\\run\\run_500\\run_\"+ str(iteration) + \"\\7_dataset_MI_1_VIF.csv\")\n",
    "\n",
    "    X_MI_2 = Dataset_MI_1_VIF.drop(['ID', 'Label', 'Sequence'], axis=1)\n",
    "    Y_MI_2 = Dataset_MI_1_VIF[['Label']]\n",
    "\n",
    "    col_MI_2 = X_MI_2.columns\n",
    "    #Calculating MI values\n",
    "    MI_score_2 = MIC(X_MI_2,Y_MI_2, random_state =99)\n",
    "    col_MI_2 = list(col_MI_2)\n",
    "    #Creating dataframe with two lists mi_score with heading \"MI Scores\" and col list with heading \"Attributes\"\n",
    "    MI_score_2_df = pd.DataFrame({'MI Scores': MI_score_2, 'Attributes': col_MI_2})\n",
    "    MI_score_2_df.to_csv(r\"C:\\Users\\Akash\\Downloads\\run\\run_500\\run_\"+ str(iteration)+ \"\\8_MI_Score_2.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# run myfunction 10 times\n",
    "\n",
    "for i in range(10):\n",
    "    t = threading.Thread(target=myfunction, args=(X,Y,i+1))\n",
    "    t.start()\n",
    "    t.join()\n",
    "    print(\"Thread\", i+1, \"is completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list \n",
    "selected_features = [list() for i in range(10)]\n",
    "#create a for loop to read the csv files\n",
    "\n",
    "for i in range(10):\n",
    "    #read the csv files\n",
    "    print(i+1)\n",
    "    dataframe = pd.read_csv(\"./run_\"+ str(i+1) + \"/8_MI_Score_2.csv\")\n",
    "    print(dataframe.shape)\n",
    "    print(i)\n",
    "    # adding these values to the list selected_feature as a new column\n",
    "    selected_features[i] = dataframe['Attributes'].to_list()\n",
    "\n",
    "for i in range(10):\n",
    "    print(len(selected_features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features = set.intersection(*map(set, selected_features))\n",
    "len(common_features)\n",
    "print(common_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = []\n",
    "for i in range(10):\n",
    "    all_values.extend(selected_features[i])\n",
    "unique_values = set(all_values)\n",
    "len(unique_values)\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(unique_values))\n",
    "print(type(common_features))\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_etc = df[['ID', 'Label', 'Sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting unique_values data type from set to list\n",
    "unique_values = list(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((unique_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./P_N.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a subset from dataframe df by using column name present in set of unique_values and matching them with the column names in df\n",
    "df_subset_unique = df[unique_values]\n",
    "df_subset_unique = pd.concat([dataset_etc, df_subset_unique], axis=1)\n",
    "df_subset_unique.to_csv(\"./9_dataset_subset_unique.csv\",index=False)\n",
    "print(df_subset_unique.shape)\n",
    "\n",
    "df_subset_common = df[common_features] \n",
    "df_subset_common = pd.concat([dataset_etc, df_subset_common], axis=1)\n",
    "df_subset_common.to_csv(\"./10_dataset_subset_common.csv\",index=False)\n",
    "print(df_subset_common.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
